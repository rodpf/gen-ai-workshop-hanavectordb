{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using langchain's vectore store plugin for HANA Vector Engine\n",
    "to store embeddings generated by AI Core.\n",
    "\n",
    "Prerequisites:\n",
    "- langchain >= 0.1.4\n",
    "- generative-ai-hub-sdk 1.2.0\n",
    "- openAI ada deployment on AI Core\n",
    "\n",
    "See:<br>\n",
    "https://pypi.org/project/generative-ai-hub-sdk/<br>\n",
    "https://github.wdf.sap.corp/AI/generative-ai-hub-sdk/blob/main/docs/gen_ai_hub/examples/gen_ai_hub.ipynb<br>\n",
    "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "import langchain_community\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, PyPDFDirectoryLoader, SitemapLoader\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print('langchain version:', langchain.__version__)\n",
    "print('langchain_community version:', langchain_community.__version__)\n",
    "# How to get the gen Ai Hub SDK version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using langchain to read and split the doc\n",
    "\n",
    "filePath = \".data/...\"\n",
    "# text_documents = TextLoader(\"data/state_of_the_union.txt\").load()\n",
    "loader = PyPDFDirectoryLoader(filePath)   \n",
    "documents = loader.load()\n",
    "#Load document \n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 10,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of document chunks: {len(text_chunks)}\")\n",
    "\n",
    "# using ai core to embed\n",
    "embeddings = OpenAIEmbeddings(proxy_model_name='text-embedding-ada-002')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection using hana-ml\n",
    "from hana_ml import ConnectionContext\n",
    "# cc = ConnectionContext(userkey='VDB_BETA', encrypt=True)\n",
    "cc= ConnectionContext(\n",
    "    address='[somehost].hanacloud.ondemand.com', \n",
    "    port='443', \n",
    "    user='[your user]', \n",
    "    password='[your password]', \n",
    "    encrypt=True\n",
    "    )\n",
    "connection = cc.connection\n",
    "\n",
    "print(cc.hana_version())\n",
    "print(cc.get_current_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a table if not exists\n",
    "db = HanaDB(\n",
    "    embedding=embeddings, connection=connection, table_name=\"PDF_SAMPLE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete already existing documents from the table\n",
    "# db.delete(filter={})\n",
    "\n",
    "# add the loaded document chunks\n",
    "db.add_documents(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the table\n",
    "hdf = cc.sql(''' SELECT \"VEC_TEXT\", \"VEC_META\", TO_NVARCHAR(\"VEC_VECTOR\") AS \"VEC_VECTOR\" FROM \"PDF_SAMPLE\" ''')\n",
    "df = hdf.head(5).collect()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Capabilities of SAP Logistics Business Network\"\n",
    "docs = db.similarity_search(query, k=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "context = \"\"\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    ##print(doc.page_content)\n",
    "    context = context + doc.page_content + \" \"\n",
    "\n",
    "print (context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate_fstring = \"\"\"\n",
    "You are an Analyzing Given context.\n",
    "You are provided multiple context items that are related to the prompt you have to answer.\n",
    "Use the following pieces of context to answer the question at the end with no more than 3 sentences. If the query is not part of the given context, reply 'not under my scope'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "promptTemplate = PromptTemplate.from_template(promptTemplate_fstring)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain import ChatOpenAI\n",
    "llm = ChatOpenAI(proxy_model_name='gpt-35-turbo', temperature=0)\n",
    "prompt = promptTemplate.format(query=query, context=context)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print (response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
